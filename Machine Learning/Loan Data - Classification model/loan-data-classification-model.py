# -*- coding: utf-8 -*-
"""ML Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPPKVBiF9ImDaZK21hVas-398SrWBebh

**Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from imblearn import under_sampling , over_sampling
from sklearn.model_selection import train_test_split
from collections import Counter

data = pd.read_csv('data.csv')
data.head()

len(data)

"""**To get the number of unique values in each column**"""

data.nunique()

"""**Check if data has null values or not**"""

data.isnull().any()

data['installment'].max()

data['int.rate'].max()

"""# Data Visualization

We have three ways to solve imbalanced problem:


1.   over sampling
2.   under sampling

We will use under sampling
"""

data['not.fully.paid'].value_counts().plot(kind= 'bar')

data['not.fully.paid'].value_counts()

zero_percentage = 100 * (data['not.fully.paid'].value_counts()[0]) / len(data)
one_percentage = 100 * (data['not.fully.paid'].value_counts()[1]) / len(data)

print(f'Percentage of zero = {zero_percentage}')
print(f'Percentage of one  = {one_percentage}')

plt.figure(figsize=(10,5))
plt.hist(data[data['not.fully.paid'] == 0]['fico'] , bins = 30 , label="1")
plt.hist(data[data['not.fully.paid'] == 1]['fico'] , bins = 30 , label = "0")
plt.legend(['1','0'])

"""**if we look at the data columns we will found that there is one categorical colum**"""

data.info()

"""**Plotting purpose column to know its values**"""

plt.figure(figsize=(10,5))
sns.countplot(data=data, x='purpose')

data.hist(bins=50, figsize=(20,15))
plt.show()

"""**Check correlation between features**"""

from pandas.plotting import scatter_matrix
attributes = ["not.fully.paid","fico", "inq.last.6mths", "int.rate",
              "revol.bal"]
scatter_matrix(data[attributes], figsize=(14, 8))

"""**Compute the relation between features and target**"""

data.corr()['not.fully.paid'].sort_values(ascending = False)

plt.figure(figsize=(14,8))
sns.countplot(data=data, x='delinq.2yrs' ,hue="not.fully.paid")

"""# Prepare the Data for Machine Learning Algorithms

# 1-Handing imbalanced

Under Sampling
"""

datacpy = data
no_frauds = len(datacpy[datacpy['not.fully.paid'] == 1])
non_fraud_indices = datacpy[datacpy["not.fully.paid"] == 0].index
random_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)
fraud_indices = datacpy[datacpy["not.fully.paid"] == 1].index
under_sample_indices = np.concatenate([fraud_indices,random_indices])
under_sample = datacpy.loc[under_sample_indices]
print(len(under_sample[under_sample['not.fully.paid'] == 1]))
print(len(under_sample[under_sample['not.fully.paid'] == 0]))

"""Over Sampling"""

from imblearn.over_sampling import RandomOverSampler

mydata = data
ros = RandomOverSampler(sampling_strategy=1)
x = mydata.drop('not.fully.paid',axis =1)
y = mydata['not.fully.paid']

x_ros , y_ros = ros.fit_resample(x,y)

targetdata = pd.DataFrame(y_ros,columns=['not.fully.paid'])
x_ros = pd.DataFrame(x_ros,columns=x.columns)
finaldata = pd.concat([x_ros,targetdata],axis=1)
x_ros

"""**Choosing Type of sampling**


1.   No Sampling
2.   Under Sampling
3.   Over Sampling


"""

typeofsampling = int(input('type'))
if typeofsampling == 1:
    data_aftersampling = data
elif typeofsampling == 2:
    data_aftersampling = under_sample
elif typeofsampling == 3:
    data_aftersampling = finaldata

"""# 2- Handling Categorical attributes"""

data_lab = data_aftersampling['not.fully.paid']

data_aftersampling.head()
data_num = data_aftersampling.drop(['purpose','not.fully.paid'],axis=1)

data_categorical = data_aftersampling[['purpose']]

"""**Converting categorical column to a numerical column using one hot encoder function**"""

cat_encoder = OneHotEncoder()
data_cat_hot = cat_encoder.fit_transform(data_categorical)
data_cat_hot.toarray()

"""# 3- Transformation Pipeline

**First handling numerical columns with standardization , applying pipeline to implement what we have done to categorical columns using onehotencoder**
"""

num_pipeline = Pipeline([
    ('std_scaler',StandardScaler())
])
data_num_tr = num_pipeline.fit_transform(data_num)

num_attrs = list(data_num)
cat_attrs = ['purpose']

fullpipeline = ColumnTransformer([
    ('num',num_pipeline,num_attrs),
    ('cat',OneHotEncoder(),cat_attrs)])

data_prepared = fullpipeline.fit_transform(data_aftersampling)

"""**Model**"""

from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import SGDClassifier
X_train, X_test, y_train, y_test = train_test_split(data_prepared,data_lab,test_size = 0.2, random_state = 0)


lin = RandomForestClassifier()
lin.fit(X_train,y_train)
y_pred =lin.predict(X_test)

len(y_test)

some_data = data.iloc[:5]
some_data

some_data_prepared = fullpipeline.transform(some_data)

labels = data_lab.iloc[:5]
labels

print("prediction" , lin.predict(some_data_prepared))
print("Some Lables" , list(labels))

from sklearn.model_selection import cross_val_predict

cv = cross_val_predict(lin , X_train , y_train , cv=3)

confusion_matrix(y_train,cv)

print(classification_report(y_test,y_pred))

accuracy_score(y_test,y_pred)

